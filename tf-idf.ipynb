{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: lxml in /usr/local/python/3.12.1/lib/python3.12/site-packages (5.3.1)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.3.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.0.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython -m pip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: nltk in /usr/local/python/3.12.1/lib/python3.12/site-packages (3.9.1)\n",
      "Requirement already satisfied: click in /usr/local/python/3.12.1/lib/python3.12/site-packages (from nltk) (8.1.8)\n",
      "Requirement already satisfied: joblib in /home/codespace/.local/lib/python3.12/site-packages (from nltk) (1.4.2)\n",
      "Requirement already satisfied: regex>=2021.8.3 in /usr/local/python/3.12.1/lib/python3.12/site-packages (from nltk) (2024.11.6)\n",
      "Requirement already satisfied: tqdm in /usr/local/python/3.12.1/lib/python3.12/site-packages (from nltk) (4.67.1)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.3.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.0.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython -m pip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install lxml\n",
    "%pip install nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lxml import etree\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tlg0012/tlg001/tlg0012.tlg001.perseus-eng3.xml\n",
      "tlg0012/tlg001/tlg0012.tlg001.perseus-eng4.xml\n",
      "tlg0012/tlg003/tlg0012.tlg003.perseus-eng1.xml\n",
      "tlg0012/tlg002/tlg0012.tlg002.perseus-eng4.xml\n",
      "tlg0012/tlg002/tlg0012.tlg002.perseus-eng3.xml\n"
     ]
    }
   ],
   "source": [
    "files = list(Path(\"tlg0012\").glob(\"./**/*perseus-eng*.xml\"))\n",
    "\n",
    "for f in files:\n",
    "    print(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "TEI_NS = \"http://www.tei-c.org/ns/1.0\"\n",
    "XML_NS = \"http://www.w3.org/XML/1998/namespace\"\n",
    "\n",
    "NAMESPACES = {\n",
    "    \"tei\": TEI_NS,\n",
    "    \"xml\": XML_NS\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tlg0012/tlg001/tlg0012.tlg001.perseus-eng3.xml\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tlg0012/tlg001/tlg0012.tlg001.perseus-eng4.xml\n",
      "tlg0012/tlg003/tlg0012.tlg003.perseus-eng1.xml\n",
      "tlg0012/tlg002/tlg0012.tlg002.perseus-eng4.xml\n",
      "tlg0012/tlg002/tlg0012.tlg002.perseus-eng3.xml\n"
     ]
    }
   ],
   "source": [
    "for file in files:\n",
    "    print(file)\n",
    "    tree = etree.parse(file)\n",
    "    text = tree.xpath(f\"//tei:div[@subtype='card']//text()\", namespaces=NAMESPACES)\n",
    "    \n",
    "    cleaned_text = []\n",
    "    for t in text:\n",
    "        if t.strip() != \"\":\n",
    "            cleaned_text.append(t)\n",
    "\n",
    "    if len(cleaned_text) > 0:\n",
    "        with open(str(file).split(\"/\")[-1].replace(\".xml\", \".txt\"), \"w+\") as f:\n",
    "            f.write('\\n'.join(cleaned_text))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/codespace/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     /home/codespace/nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "\n",
    "nltk.download(\"punkt\")\n",
    "nltk.download(\"punkt_tab\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 200630 tokens in tlg0012.tlg001.perseus-eng3.txt.\n",
      "There are 135463 tokens in tlg0012.tlg002.perseus-eng4.txt.\n",
      "There are 175611 tokens in tlg0012.tlg001.perseus-eng4.txt.\n",
      "There are 152631 tokens in tlg0012.tlg002.perseus-eng3.txt.\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "tokenized_texts = {}\n",
    "\n",
    "text_files = Path(\".\").glob(\"tlg0012.tlg00*.perseus-eng[1-4].txt\")\n",
    "\n",
    "for file in text_files:\n",
    "    name = str(file)\n",
    "\n",
    "    with open(file) as f:\n",
    "        text = f.read().lower()\n",
    "        tokens = word_tokenize(text)\n",
    "        print(f\"There are {len(tokens)} tokens in {name}.\")\n",
    "        \n",
    "        tokenized_texts[name] = tokens\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "for filename, tokens in tokenized_texts.items():\n",
    "    counts = Counter(tokens)\n",
    "\n",
    "    tokenized_texts[filename] = {\"tokens\": tokens, \"counts\": counts}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "128"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_texts[\"tlg0012.tlg001.perseus-eng3.txt\"][\"counts\"][\"odysseus\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0\n"
     ]
    }
   ],
   "source": [
    "df_achilles = 0\n",
    "df_odysseus = 0\n",
    "\n",
    "for filename, values in tokenized_texts.items():\n",
    "    if \"odysseus\" in values['counts']:\n",
    "        df_odysseus += 1\n",
    "    \n",
    "    if \"achilles\" in values[\"counts\"]:\n",
    "        df_achilles += 1\n",
    "\n",
    "from math import log10\n",
    "\n",
    "n_docs = len(tokenized_texts.keys())\n",
    "\n",
    "idf_achilles = log10(n_docs / df_achilles)\n",
    "idf_odysseus = log10(n_docs / df_odysseus)\n",
    "\n",
    "print(idf_achilles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In tlg0012.tlg001.perseus-eng3.txt:\n",
      "TF of achilles: 0.002043562777251657\n",
      "TF of odysseus: 0.000637990330459054\n",
      "TF-IDF of achilles: 0.0\n",
      "TF-IDF of odysseus: 0.0\n",
      "\n",
      "In tlg0012.tlg002.perseus-eng4.txt:\n",
      "TF of achilles: 0.0001254955227626732\n",
      "TF of odysseus: 0.0042816119530794386\n",
      "TF-IDF of achilles: 0.0\n",
      "TF-IDF of odysseus: 0.0\n",
      "\n",
      "In tlg0012.tlg001.perseus-eng4.txt:\n",
      "TF of achilles: 0.002403038534032606\n",
      "TF of odysseus: 0.0007061061095261686\n",
      "TF-IDF of achilles: 0.0\n",
      "TF-IDF of odysseus: 0.0\n",
      "\n",
      "In tlg0012.tlg002.perseus-eng3.txt:\n",
      "TF of achilles: 0.0001048279838302835\n",
      "TF of odysseus: 0.0041603606082643765\n",
      "TF-IDF of achilles: 0.0\n",
      "TF-IDF of odysseus: 0.0\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for filename, values in tokenized_texts.items():\n",
    "    total_terms = len(values['tokens'])\n",
    "\n",
    "    tf_achilles = values['counts']['achilles'] / total_terms\n",
    "    tf_odysseus = values['counts']['odysseus'] / total_terms\n",
    "\n",
    "    tf_idf_achilles = tf_achilles * idf_achilles\n",
    "    tf_idf_odysseus = tf_odysseus * idf_odysseus\n",
    "\n",
    "    print(f\"\"\"In {filename}:\n",
    "TF of achilles: {tf_achilles}\n",
    "TF of odysseus: {tf_odysseus}\n",
    "TF-IDF of achilles: {tf_idf_achilles}\n",
    "TF-IDF of odysseus: {tf_idf_odysseus}\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{1, 2, 3}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_list = [1, 1, 2, 3, 3]\n",
    "\n",
    "set(my_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "non_universal_terms = {}\n",
    "\n",
    "for filename, values in tokenized_texts.items():\n",
    "    my_set = set(values['counts'].keys())\n",
    "\n",
    "    for other_file, other_values in tokenized_texts.items():\n",
    "        if other_file != filename:\n",
    "            my_set -= set(other_values['counts'].keys())\n",
    "    \n",
    "    non_universal_terms[filename] = my_set\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6020599913279624\n"
     ]
    }
   ],
   "source": [
    "idf_non_universal = log10(n_docs / 1)\n",
    "print(idf_non_universal)\n",
    "tf_idf_non_universal = {}\n",
    "\n",
    "for filename, values in tokenized_texts.items():\n",
    "    total_terms = len(values['tokens']) \n",
    "    tf_idf_non_universal[filename] = {}\n",
    "\n",
    "    for word in non_universal_terms[filename]: \n",
    "        tf = values['counts'][word] / total_terms \n",
    "        tf_idf = tf * idf_non_universal \n",
    "\n",
    "        tf_idf_non_universal[filename][word] = tf_idf\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Highest TF IDFs in tlg0012.tlg001.perseus-eng3.txt\n",
      "hector: 0.0014464083926634993\n",
      "athene: 0.0004891381078924281\n",
      "drave: 0.00025507201945310673\n",
      "honour: 0.00018905337912406733\n",
      "valour: 0.0001830516845487001\n",
      "\n",
      "Lowest TF IDFs in tlg0012.tlg001.perseus-eng3.txt\n",
      "shallow: 3.0008472876836084e-06\n",
      "pheneos: 3.0008472876836084e-06\n",
      "burgeoneth: 3.0008472876836084e-06\n",
      "quick-glancing: 3.0008472876836084e-06\n",
      "wandereth: 3.0008472876836084e-06\n",
      "\n",
      "Highest TF IDFs in tlg0012.tlg002.perseus-eng4.txt\n",
      "eumaios: 0.0003155567157399831\n",
      "alkinoos: 0.0002755565686743514\n",
      "antinoos: 0.00025777872553407066\n",
      "eurykleia: 0.00014666720590731608\n",
      "eurymakhos: 0.00012444490198196517\n",
      "\n",
      "Lowest TF IDFs in tlg0012.tlg002.perseus-eng4.txt\n",
      "ridiculous: 4.444460785070184e-06\n",
      "tyndarus: 4.444460785070184e-06\n",
      "enabled: 4.444460785070184e-06\n",
      "mixing-jugs: 4.444460785070184e-06\n",
      "telemos: 4.444460785070184e-06\n",
      "\n",
      "Highest TF IDFs in tlg0012.tlg001.perseus-eng4.txt\n",
      "hektor: 0.0016421906136067444\n",
      "therapôn: 0.00012684979687567754\n",
      "pontos: 0.00011656467821008207\n",
      "menoitios: 0.00011313630532155025\n",
      "ajaxes: 0.00010970793243301841\n",
      "\n",
      "Lowest TF IDFs in tlg0012.tlg001.perseus-eng4.txt\n",
      "mantinea: 3.4283728885318253e-06\n",
      "unnoticed: 3.4283728885318253e-06\n",
      "feature: 3.4283728885318253e-06\n",
      "eeriboia: 3.4283728885318253e-06\n",
      "cloud-compelling: 3.4283728885318253e-06\n",
      "\n",
      "Highest TF IDFs in tlg0012.tlg002.perseus-eng3.txt\n",
      "“: 0.003234527670584148\n",
      "”: 0.0021852784506141685\n",
      "antinous: 0.00022878366450473244\n",
      "eumaeus: 0.00017356002134841773\n",
      "eurymachus: 0.00012622547007157652\n",
      "\n",
      "Lowest TF IDFs in tlg0012.tlg002.perseus-eng3.txt\n",
      "signed: 3.944545939736766e-06\n",
      "seeks: 3.944545939736766e-06\n",
      "amnisus: 3.944545939736766e-06\n",
      "quaffs: 3.944545939736766e-06\n",
      "us—: 3.944545939736766e-06\n"
     ]
    }
   ],
   "source": [
    "for filename, tf_idfs in tf_idf_non_universal.items():\n",
    "    highest_tf_idfs = sorted(tf_idfs.items(), key=lambda x: x[1], reverse=True)[:5] \n",
    "    lowest_tf_idfs = sorted(tf_idfs.items(), key=lambda x: x[1], reverse=False)[:5] \n",
    "\n",
    "    print(f\"\\nHighest TF IDFs in {filename}\")\n",
    "    for word, tf_idf in highest_tf_idfs:\n",
    "        print(f\"{word}: {tf_idf}\")\n",
    "\n",
    "    print(f\"\\nLowest TF IDFs in {filename}\")\n",
    "    for word, tf_idf in lowest_tf_idfs:\n",
    "        print(f\"{word}: {tf_idf}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TF-IDF is helpful in showing how important a specific word is in document relative to the entire corpus. If a token's TF_IDF score is high, it means that the term is frequently used in not many documents. If the TF-IDF score is low, it means the term either has a low frequency or it is frequent in all documents (like we saw with 'achilles' and 'odysseus'). \n",
    "\n",
    "I explored the non universal word set we created to look at the TF-IDF scores for words that only appear in one document. I listed the 5 highest and 5 lowest TF-IDF scores in each document. The high TF-IDF scores make some sense, like 'hector' being used very frequently in the first document and is thus important to the text. The low scores also make sense, with words like 'quick-glancing' that would not occur very often. Although, the scores also revealed some issues with the tokenization with tokens like '\"' and 'us—', showing up, which would mess up the TF-IDF scores. In the future or if I had more time I would go back and fix up the tokenization to hopefully fix the issue. \n",
    "\n",
    "One thing I thought TF-IDF might be useful for would be for making something like a search engine. Like if someone searched a word, TF-IDF could be useful to help find the most relevant documents. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
